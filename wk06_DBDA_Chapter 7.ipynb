{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Chapter 7  Markov Chain Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### text: Doing Bayesian Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This chapter is about methods for approximating a posterior distribution by collecting from it a large\n",
    "representative sample.   \n",
    "These methods are important because complex posterior distributions are otherwise very challenging to get a handle on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Terminology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Monte Carlo simulation**  \n",
    "Any simulation that samples a lot of random\n",
    "values from a distribution.    \n",
    "Monte Carlo(모나코에 있는 도박장)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic3/07_A.png\"  width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic3/07_B.png\"  width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The **Metropolis algorithm** and **Gibbs sampling** are specific types of Monte Carlo\n",
    "process.   \n",
    "They generate random walks such that each step in the walk is completely\n",
    "independent of the steps before the current position. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Markov process**  \n",
    "any such process in which each step\n",
    "has no memory for states before the current one "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**Markov chain**  \n",
    "a succession of such Markov process steps \n",
    "It was named after the mathematician AndreyMarkov (1856–1922)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Metropolis algorithm and Gibbs sampling are examples of a\n",
    "**Markov chain Monte Carlo (MCMC)** process.   \n",
    "There are many others. \n",
    "\n",
    "It is the invention of MCMC algorithms, along with software for automatically creating samplers for complex\n",
    "models (such as Pymc3 and Stan), and fast cheap computers, that allow us to do\n",
    "Bayesian data analysis for complex realistic data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This chapter introduces the methods we will use for producing accurate approximations to Bayesian posterior distributions for realistic applications.   \n",
    "The class of methods is\n",
    "called Markov chain Monte Carlo (MCMC).   \n",
    "It is MCMC algorithms and software, along with fast computer hardware,\n",
    "that allow us to do Bayesian data analysis for realistic applications that would have been\n",
    "effectively impossible 30 years ago.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### grid approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We already recognized the possibility that our prior beliefs about $\\theta$ could not be adequately\n",
    "represented by any function that yields an analytically solvable\n",
    "posterior function.   \n",
    "Grid approximation is one approach to addressing such situations.\n",
    "When we have just one parameter with a finite range, then approximation by a grid is\n",
    "a useful procedure.  \n",
    "But what if we have several parameters? \n",
    "It is much more typical to have models involving several parameters.   \n",
    "In these situations, the parameter space cannot be spanned by a grid with a reasonable number of\n",
    "points.   \n",
    "Consider, for example, a model with six parameters.   \n",
    "The parameter space is a six-dimensional space that involves the joint distribution of all combinations of\n",
    "parameter values.   \n",
    "If we set up a comb on each parameter that has 1,000 values, then the\n",
    "six-dimensional parameter space has $1,000^6$ = 1,000,000,000,000,000,000 combinations\n",
    "of parameter values, which is too many for any computer to evaluate.   \n",
    "In anticipation of\n",
    "those situations when grid approximation will not work, we explore a new method called\n",
    "Markov chain Monte Carlo, or MCMC for short.   \n",
    "In this chapter, we apply MCMC to\n",
    "the simple situation of estimating a single parameter. In real research you would probably\n",
    "not want to apply MCMC to such simple one-parameter models, instead going with\n",
    "mathematical analysis or grid approximation.   \n",
    "But it is very useful to learn about MCMC\n",
    "in the one-parameter context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The method described in this chapter assumes that the prior distribution is specified by a function that is easily evaluated.   \n",
    "This simply means that if you specify a value for\n",
    "$\\theta$, then the value of $p(\\theta)$ is easily determined, especially by a computer.   \n",
    "The method also assumes that the value of the likelihood function, $p(D|\\theta)$, can be computed for any specified values of $D$ and $\\theta$. (Actually, all that the method really demands is that the\n",
    "prior and likelihood can be easily computed up to a multiplicative constant.)   \n",
    "In other words, the method does not require evaluating the difficult integral in the denominator of Bayes’ rule.  \n",
    "What the method produces for us is an approximation of the posterior distribution, $p(\\theta|D)$, in the form of a large number of $\\theta$ values sampled from that distribution.\n",
    "This heap of representative $\\theta$ values can be used to estimate the central tendency\n",
    "of the posterior, its highest density interval (HDI), etc.   \n",
    "The posterior distribution is\n",
    "estimated by randomly generating a lot of values from it, and therefore, by analogy\n",
    "to the random events at games in a casino, this approach is called a Monte Carlo\n",
    "method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 7.1. APPROXIMATING A DISTRIBUTION WITH A LARGE SAMPLE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic3/07_01.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 7.2. A SIMPLE CASE OF THE METROPOLIS ALGORITHM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Our goal in Bayesian inference is to get an accurate representation of the posterior distribution.   \n",
    "One way to do that is to sample a large number of representative points\n",
    "from the posterior.   \n",
    "The question then becomes this: How can we sample a large number\n",
    "of representative values from a distribution?   \n",
    "For an answer, let’s ask a politician."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.2.1.  A politician stumbles upon the Metropolis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Suppose an elected politician lives on a long chain of islands.   \n",
    "He is constantly traveling from island to island, wanting to stay in the public eye.  \n",
    "At the end of every day, he has to decide whether to   \n",
    "(i) stay on the current island  \n",
    "(ii) move to the adjacent island to the west   \n",
    "(iii) move to the adjacent island to the east.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "His goal is to visit all the islands proportionally to their relative population, so\n",
    "that he spends the most time on the most populated islands, and proportionally less time\n",
    "on the less populated islands.   \n",
    "Unfortunately, he holds his office despite having no idea\n",
    "what the total population of the island chain is, and he doesn’t even know exactly how\n",
    "many islands there are!   \n",
    "His entourage of advisers is capable of some minimal information\n",
    "gathering abilities, however.   \n",
    "When they are not busy fundraising, they can ask the mayor\n",
    "of the island they are on how many people are on the island. And, when the politician\n",
    "proposes to visit an adjacent island, they can ask the mayor of that adjacent island how\n",
    "many people are on that island.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The politician has a simple heuristic for deciding whether to travel to the proposed island:   \n",
    "First, he flips a (fair) coin to decide whether to propose the adjacent island to the east or the adjacent island to the west.   \n",
    "If the proposed island has a larger population than\n",
    "the current island, then he definitely goes to the proposed island.  \n",
    "On the other hand,\n",
    "if the proposed island has a smaller population than the current island, then he goes to\n",
    "the proposed island only probabilistically, to the extent that the proposed island has a\n",
    "population as big as the current island.   \n",
    "If the population of the proposed island is only\n",
    "half as big as the current island, the probability of going there is only 50%.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In more detail, denote the population of the proposed island as $P_{proposed}$, and the\n",
    "population of the current island as $P_{current}$.   \n",
    "Then he moves to the less populated island\n",
    "with probability $p_{move}=\\frac{P_{proposed}}{P_{current}}$.   \n",
    "The politician does this by spinning a fair spinner\n",
    "marked on its circumference with uniform values from zero to one.   \n",
    "If the pointed-to\n",
    "value is between zero and $p_{move}$, then he moves.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "What’s amazing about this heuristic is that it works: In the long run, the probability\n",
    "that the politician is on any one of the islands exactly matches the relative population of\n",
    "the island!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.2.2. A random walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Suppose that there are seven islands in the chain, with relative populations as shown in the bottom panel of Figure 7.2.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic3/07_02.png\"  width=\"450\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The islands are indexed by the value $\\theta$, whereby the leftmost, western island is $\\theta$ = 1 and the rightmost, eastern island is $\\theta$ = 7.   \n",
    "The relative populations of the islands\n",
    "increase linearly such that $P(\\theta)$ = $\\theta$.   \n",
    "Notice that uppercase $P( )$ refers to the relative\n",
    "population of the island, not its absolute population and not its probability mass.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The middle panel of Figure 7.2 shows one possible trajectory taken by the politician. \n",
    "Each day corresponds to one-time increment, indicated on the vertical axis.   \n",
    "The plot of the trajectory shows that on the first day (t = 1), the politician happens to start on the middle island in the chain, hence $\\theta_{current}$=4.   \n",
    "To decide where to go on the second day, he\n",
    "flips a coin to propose moving either one position left or one position right.   \n",
    "In this case,\n",
    "the coin proposed moving right, hence $\\theta_{proposed} = 5$.   \n",
    "Because the relative population at\n",
    "the proposed position is greater than the relative population at the current position (i.e., $P(5) > P(4)$), the proposed move is accepted. The trajectory shows this move, because\n",
    "when $t = 2$, then $\\theta = 5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Consider the next day, when $t = 2$ and $\\theta = 5$.  \n",
    "The coin flip proposes moving to the left.   \n",
    "The probability of accepting this proposal is $p_{move}=\\frac{P_{proposed}}{P_{current}} = \\frac 4 5\n",
    "= 0.80$.   \n",
    "The politician then spins a fair spinner that has a circumference marked from 0\n",
    "to 1, which happens to come up with a value greater than 0.80.   \n",
    "Therefore, the politician\n",
    "rejects the proposed move and stays at the current island.   \n",
    "Hence the trajectory shows that $\\theta$ is still 5 when $t = 3$.   \n",
    "The middle panel of Figure 7.2 shows the trajectory for the\n",
    "first 500 steps in this random walk across the islands.   \n",
    "The scale of the time step is plotted\n",
    "logarithmically so you can see the details of the early steps but also see the trend of the\n",
    "later steps.   \n",
    "There are many thousands of steps in the simulation.  \n",
    "The upper panel of Figure 7.2 shows a histogram of the frequencies with which each\n",
    "position is visited during this simulation.  \n",
    "Notice that the sampled relative frequencies closely\n",
    "mimic the actual relative populations in the bottom panel!   \n",
    "In fact, a sequence generated\n",
    "this way will converge, as the sequence gets longer, to an arbitrarily close approximation\n",
    "of the actual relative probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.2.3. General properties of a random walk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The trajectory shown in Figure 7.2 is just one possible sequence of positions when the\n",
    "movement heuristic is applied.   \n",
    "At each time step, the direction of the proposed move is random, and if the relative probability of the proposed position is less than that of the current position, then acceptance of the proposed move is also random.   \n",
    "Because of the randomness, if the process were started over again, then the specific trajectory would\n",
    "almost certainly be different.   \n",
    "Regardless of the specific trajectory, in the long run the relative frequency of visits mimics the target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<br><br>\n",
    "<img style=\"float: left;\" src=\"pic3/07_03.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 7.3 shows the probability of being in each position as a function of time.  \n",
    "At\n",
    "time t = 1, the politician starts at $\\theta = 4$.     \n",
    "This starting position is indicated in the upperleft panel of Figure 7.3, labeled $t = 1$, by the fact that there is 100% probability of being at $\\theta = 4$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We want to derive the probability of ending up in each position at the next time step.\n",
    "To determine the probabilities of positions for time $t = 2$, consider the possibilities from\n",
    "the movement process.   \n",
    "The process starts with the flip of a fair coin to decide which\n",
    "direction to propose moving.   \n",
    "There is a 50% probability of proposing to move right,\n",
    "to $\\theta = 5$.  \n",
    "By inspecting the target distribution of relative probabilities in the lower-right\n",
    "panel of Figure 7.3, you can see that $P(\\theta=5) > P(\\theta=4)$, and therefore, a rightward move\n",
    "is always accepted whenever it is proposed.   \n",
    "Thus, at time $t = 2$, there is a 50% probability\n",
    "of ending up at $\\theta = 5$.   \n",
    "The panel labeled $t = 2$ in Figure 7.3 plots this probability as a\n",
    "bar of height 0.5 at $\\theta = 5$.  \n",
    "The other 50% of the time, the proposed move is to the left, to $\\theta = 3$.  \n",
    "By inspecting the target distribution of relative probabilities in the lower-right\n",
    "panel of Figure 7.3, you can see that $P(\\theta=3) = 3$, whereas $P(\\theta=4) = 4$, and therefore,\n",
    "a leftward move is accepted only 3/4 of the times it is proposed. Hence, at time $t = 2$, the probability of ending up at θ = 3 is 50%·3/4 = 0.375.   \n",
    "The panel labeled $t = 2$ in Figure 7.3 shows this as a bar of height 0.375 at $\\theta  = 3$.  \n",
    "Finally, if a leftward move is\n",
    "proposed but not accepted, we just stay at $\\theta  = 5$.  \n",
    "The probability of this happening is\n",
    "only 50%·(1 − 3/4) = 0.125."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This process repeats for the next time step. I won’t go through the arithmetic details\n",
    "for each value of $\\theta$.   \n",
    "But it is important to notice that after two proposed moves, i.e., when\n",
    "$t = 3$, the politician could be at any of the positions $\\theta = 2$ through $\\theta = 6$, but not yet at\n",
    "$\\theta = 1$ or $\\theta = 7$, because he could be at most two positions away from where he started.  \n",
    "The probabilities continue to be computed the same way at every time step.   \n",
    "You can\n",
    "see that in the early time steps, the probability distribution is not a straight incline like the target distribution.  \n",
    "Instead, the probability distribution has a bulge over the starting position.   \n",
    "As you can see in Figure 7.3, by time $t = 99$, the position probability is virtually\n",
    "indistinguishable from the target distribution, at least for this simple distribution.   \n",
    "More complex distributions require a longer duration to achieve a good approximation to the target."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The graphs of Figure 7.3 show the probability that the moving politician is at each\n",
    "value of $\\theta$.   \n",
    "But remember, at any given time step, the politician is at only one particular\n",
    "position, as shown in Figure 7.2.   \n",
    "To approximate the target distribution, we let the politician meander around for many time steps while we keep track of where he has been.  \n",
    "When we have a long record of where the traveler has been, we can approximate\n",
    "the target probability at each value of $\\theta$ by simply counting the relative number times\n",
    "that the traveler visited that value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### summary of the algorithm for moving from one position to another"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We are currently at position $\\theta_{current}$.   \n",
    "We then propose to move one position right or one position left.   \n",
    "The specific proposal is determined by flipping a coin, which can result in\n",
    "50% heads (move right) or 50% tails (move left).   \n",
    "The range of possible proposed moves,\n",
    "and the probability of proposing each, is called the proposal distribution.   \n",
    "In the present algorithm, the proposal distribution is very simple: It has only two values with 50-50\n",
    "probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Having proposed a move, we then decide whether or not to accept it.   \n",
    "The acceptance decision is based on the value of the target distribution at the proposed position, relative\n",
    "to the value of the target distribution at our current position.   \n",
    "Specifically, if the target distribution is greater at the proposed position than at our current position, then we\n",
    "definitely accept the proposed move: We always move higher if we can.   \n",
    "On the other hand, if the target position is less at the proposed position than at our current position,\n",
    "we accept the move probabilistically: We move to the proposed position with probability\n",
    "$p_{move}=\\frac{P_{proposed}}{P_{current}}$, where $P(\\theta)$ is the value of the target distribution at $\\theta$.   \n",
    "We can combine these two possibilities, of the target distribution being higher or lower\n",
    "at the proposed position than at our current position, into a single expression for the\n",
    "probability of moving to the proposed position:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;$\\large p_{move}= min(\\frac{P(\\theta_{proposed})}{P(\\theta_{current})},1)$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;(7.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Notice that Equation 7.1 says that when $P(\\theta_{proposed}) > P(\\theta_{current})$, then $p_{move} = 1$.   \n",
    "Notice also that the target distribution, $P(\\theta)$, does not need to be normalized, which means\n",
    "it does not need to sum to 1 as a probability distribution must.   \n",
    "This is because what\n",
    "matters for our choice is the ratio, $\\frac{P_{proposed}}{P_{current}}$, not the absolute magnitude of\n",
    "$P(\\theta)$.   \n",
    "This property was used in the example of the island-hopping politician: The target\n",
    "distribution was the population of each island, not a normalized probability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Having proposed a move by sampling from the proposal distribution and having\n",
    "then determined the probability of accepting the move according to Equation 7.1, we\n",
    "then actually accept or reject the proposed move by sampling a value from a uniform\n",
    "distribution over the interval [0, 1].   \n",
    "If the sampled value is between 0 and pmove, then we actually make the move.   \n",
    "Otherwise, we reject the move and stay at our current position.  \n",
    "The whole process repeats at the next time step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.2.4. Why we care"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Notice what we must be able to do in the random-walk process:  \n",
    "• We must be able to generate a random value from the proposal distribution, to create\n",
    "$\\theta_{proposed}$.  \n",
    "• We must be able to evaluate the target distribution at any proposed position, to\n",
    "compute $\\frac{P_{proposed}}{P_{current}}$.  \n",
    "• We must be able to generate a random value from a uniform distribution, to accept\n",
    "or reject the proposal according to $p_{move}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "By being able to do those three things, we are able to do indirectly something we could not do directly: We can generate random samples from the target distribution.  \n",
    "Moreover, we can generate those random samples from the target distribution even when\n",
    "the target distribution is not normalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This technique is profoundly useful when the target distribution $P(\\theta)$ is a posterior proportional to $p(D|\\theta)p(\\theta)$.   \n",
    "Merely by evaluating $p(D|\\theta)p(\\theta)$, without normalizing it\n",
    "by $p(D)$, we can generate random representative values from the posterior distribution.  \n",
    "This result is wonderful because the method obviates direct computation of the evidence $p(D)$, which is one of the most difficult aspects of Bayesian\n",
    "inference.   \n",
    "By using MCMC techniques, we can do Bayesian inference in rich and\n",
    "complex models.  \n",
    "It has only been with the development of MCMC algorithms and\n",
    "software that Bayesian inference is applicable to complex data analysis, and it has only been with the production of fast and cheap computer hardware that Bayesian inference is accessible to a wide audience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.2.5. Why it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To get an intuition for why this algorithm works, consider two adjacent positions and the probabilities of moving from one to the other.   \n",
    "We’ll see that the relative transition probabilities, between adjacent positions, exactly match the relative values of the target\n",
    "distribution.  \n",
    "Extrapolate that result across all the positions, and you can see that, in\n",
    "the long run, each position will be visited proportionally to its target value.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now the details:   \n",
    "Suppose we are at position $\\theta$.   \n",
    "The probability of moving to $\\theta + 1$, denoted\n",
    "$p(\\theta → \\theta + 1)$, is the probability of proposing that move times the probability of\n",
    "accepting it if proposed, which is $p(\\theta → \\theta + 1)=0.5·min(\\frac{P(\\theta+1)}{P(\\theta)},1)$.    \n",
    "On the other hand, if we are presently at position $\\theta + 1$, the probability of moving to $\\theta$ is\n",
    "the probability of proposing that move times the probability of accepting it if proposed,\n",
    "which is $p(\\theta+1 → \\theta )=0.5·min(\\frac{P(\\theta)}{P(\\theta+1)},1)$.   \n",
    "The ratio of the transition probabilities is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_04_01.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Equation 7.2 tells us that during transitions back and forth between adjacent positions, the relative probability of the transitions exactly matches the relative values of the target distribution.   \n",
    "That might be enough for you to get the intuition that, in the\n",
    "long run, adjacent positions will be visited proportionally to their relative values in the\n",
    "target distribution.   \n",
    "If that’s true for adjacent positions, then, by extrapolating from one\n",
    "position to the next, it must be true for the whole range of positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To make that intuition more defensible, we have to fill in some more details.   \n",
    "To do this, I’ll use matrix arithmetic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_04_02.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The usefulness of putting the transition probabilities into a matrix is that we can then use matrix multiplication to get from any current location to the probability of the next\n",
    "locations.   \n",
    "Here’s a reminder of how matrixmultiplication operates.   \n",
    "Consider a matrix $T$.  \n",
    "The value in its $r$th row and $c$th column is denoted $T_{rc}$.  \n",
    "We can multiply the matrix on its left side by a row vector $w$, which yields another row vector.   \n",
    "The $c$th component of the product $wT$ is $\\sum_r w_rT_{rc}$.   \n",
    "In other words, to compute the $c$th component of the result,\n",
    "take the row vector $w$ and multiply its components by the corresponding components\n",
    "in the $c$th column of $T$, and sum up those component products. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To use the transition matrix in Equation 7.3, we put the current location probabilities into a row vector, which I will denote $w$ because it indicates where we are.   \n",
    "For example, if at the current time, we are definitely in location $\\theta =4$, then $w$ has 1.0 in its $\\theta =4$ component, and zeros everywhere else.   \n",
    "To determine the probability of the locations at the next time step, we simply multiply $w$ by $T$.   \n",
    "Here’s a key example to think through:  \n",
    "When $w =[. . . , 0, 1, 0, . . .]$ with a 1 only in the $\\theta$ position, then $wT$ is simply the row of $T$ corresponding to $\\theta$, because the $c$th component of $wT$ is $\\sum_r w_rT_{rc} = T_{\\theta c}$, where\n",
    "I’m using the subscript $\\theta$ to stand for the index that corresponds to value $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Matrix multiplication is a very useful procedure for keeping track of position probabilities.   \n",
    "At every time step, we just multiply the current position probability vector\n",
    "$w$ by the transition probability matrix $T$ to get the position probabilities for the next time step.   \n",
    "We keep multiplying by $T$, over and over again, to derive the long-run position\n",
    "probabilities.   \n",
    "This process is exactly what generated the graphs in Figure 7.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "*Here’s the climactic implication:   \n",
    "When the vector of position probabilities is the target\n",
    "distribution, it stays that way on the next time step!   \n",
    "In other words, the position probabilities\n",
    "are stable at the target distribution.* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can actually prove this result without much trouble.   \n",
    "Suppose the current position probabilities are the target probabilities, i.e., \n",
    "$w =[. . . ,P(\\theta-1), P(\\theta), P(\\theta+1), . . .]/Z$, where $Z =\\sum_\\theta P(\\theta)$ is the normalizer for\n",
    "the target distribution.   \n",
    "Consider the $\\theta$ component of $wT$.   \n",
    "We will demonstrate that the\n",
    "$\\theta$ component of $wT$ is the same as the $\\theta$ component of $w$, for any component $\\theta$.   \n",
    "The $\\theta$ component of $wT$ is  $\\sum_r w_rT_{r \\theta}$.  \n",
    "Look back at the transition matrix in Equation 7.3, and\n",
    "you can see then that the $\\theta$ component of $wT$ is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_04_03.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "To simplify that equation, we can consider separately the four cases:   \n",
    "Case 1: $P(\\theta) > P(\\theta −1)$ and $P(\\theta) > P(\\theta +1)$;   \n",
    "Case 2: $P(\\theta) > P(\\theta −1)$ and $P(\\theta) < P(\\theta +1)$;  \n",
    "Case 3: $P(\\theta) < P(\\theta −1)$ and $P(\\theta) > P(\\theta +1)$;  \n",
    "Case 4: $P(\\theta) < P(\\theta −1)$ and $P(\\theta) < P(\\theta +1)$;  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In each case, Equation 7.4 simplifies to $P(\\theta)/Z$.   \n",
    "For example, consider  \n",
    "Case 1: $P(\\theta) > P(\\theta −1)$ and $P(\\theta) > P(\\theta +1)$.   \n",
    "Equation 7.4 becomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_04_04.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If you work through the other cases, you’ll find that it always reduces to $P(\\theta)/Z$.   \n",
    "In conclusion, when the $\\theta$ component starts at $P(\\theta)/Z$, it stays at $P(\\theta)/Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 7.3. THE METROPOLIS ALGORITHM MORE GENERALLY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the previous\n",
    "section, we considered the simple case of   \n",
    "(i) discrete positions,   \n",
    "(ii) on one dimension,\n",
    "and (iii) with moves that proposed just one position left or right.   \n",
    "\n",
    "That simple situation made it relatively easy (believe it or not) to understand the procedure and how it works.  \n",
    "The general algorithm applies to   \n",
    "(i) continuous values,   \n",
    "(ii) on any number of dimensions,  \n",
    "and (iii) with more general proposal distributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The essentials of the general method are the same as for the simple case.   First, we have some target distribution, $P(\\theta)$, over a multidimensional continuous parameter space\n",
    "from which we would like to generate representative sample values.   \n",
    "We must be able\n",
    "to compute the value of  $P(\\theta)$ for any candidate value of  $\\theta$.   The distribution,  $P(\\theta)$,\n",
    "does not have to be normalized, however.   \n",
    "It merely needs to be nonnegative.   \n",
    "In typical\n",
    "applications,  $P(\\theta)$ is the unnormalized posterior distribution on $\\theta$, which is to say, it is\n",
    "the product of the likelihood and the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sample values from the target distribution are generated by taking a random walk through the parameter space.   \n",
    "The walk starts at some arbitrary point, specified by the user. The starting point should be someplace where $P(\\theta)$ is nonzero.   \n",
    "The random walk progresses at each time step by proposing a move to a new position in parameter space\n",
    "and then deciding whether or not to accept the proposed move.   \n",
    "Proposal distributions can take on many different forms, with the goal being to use a proposal distribution\n",
    "that efficiently explores the regions of the parameter space where $P(\\theta)$ has most of its mass.   \n",
    "Of course, we must use a proposal distribution for which we have a quick way to generate random values!  \n",
    "For our purposes,we will consider the generic case in which the proposal distribution is normal, centered at the current position.  \n",
    "The idea behind using a normal distribution is that the proposed move will typically be near the current position,\n",
    "with the probability of proposing a more distant position dropping off according to the normal curve.   \n",
    "Computer languages such as Python have built-in functions for generating\n",
    "pseudorandom values from a normal distribution.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Having generated a proposed new position, the algorithm then decides whether or not to accept the proposal.  \n",
    "The decision rule is exactly what was already specified in Equation 7.1.   \n",
    "In detail, this is accomplished by computing the ratio $p_{move} =\\frac{P(\\theta_{proposed})}{P(\\theta_{current})}$.  \n",
    "Then a random number from the uniform interval [0, 1] is generated.  \n",
    "If the random number is between 0 and $p_{move}, then the move is accepted.   \n",
    "The process repeats and, in the long run, the positions visited by the random walk will closely approximate the\n",
    "target distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.3.1. Metropolis algorithmapplied to Bernoulli likelihood and beta prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "In the scenario of the island-hopping politician, the islands represented candidate parameter values, and the relative populations represented relative posterior probabilities.  \n",
    "In the scenario of coin flipping, the parameter $\\theta$ has values that range on a continuum from zero to one, and the relative posterior probability is computed as likelihood times prior.   \n",
    "To apply the Metropolis algorithm, we conceive of the parameter dimensionas a dense chain of infinitesimal islands, and we think of the (relative) population of each infinitesimal island as its (relative) posterior probability density.   \n",
    "And, instead of the proposed jump being only to immediately adjacent islands, the proposed jump can be to islands farther away from the current island.  \n",
    "We need a proposal distribution that will let\n",
    "us visit any parameter value on the continuum.   \n",
    "For this purpose, we will use the familiar normal distribution. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We will apply the Metropolis algorithm to the following familiar scenario.   We flip a coin N times and observe z heads.   \n",
    "We use a Bernoulli likelihood function,\n",
    "$p(z,N|\\theta)=\\theta^z (1−\\theta)^{(N−z)}$.  \n",
    "We start with a prior $p(\\theta) = beta(\\theta|a, b)$.   \n",
    "For the proposed jump in the Metropolis algorithm, we will use a normal distribution centered at zero with standard deviation (SD) denoted as $\\sigma$.  \n",
    "We denote the proposed jump as $\\Delta\\theta ∼\n",
    "normal(\\mu=0, \\sigma)$, where the symbol “$∼$” means that the value is randomly sampled from the distribution.   \n",
    "Thus, the proposed jump is usually close to the current position, because the mean jump is zero, but the proposed jump can be positive or negative, with larger magnitudes less likely than smaller magnitudes.   \n",
    "Denote the current parameter value as $\\theta_{cur}$ and the proposed parameter value as $\\theta_{pro} = \\theta_{cur}+\\Delta\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Metropolis algorithm then proceeds as follows.   \n",
    "Start at an arbitrary initial value of $\\theta$ (in the valid range).  \n",
    "This is the current value, denoted $\\theta_{cur}$.   \n",
    "Then:  \n",
    "**1.** Randomly generate a proposed jump, $\\Delta\\theta ∼\n",
    "normal(\\mu=0, \\sigma)$ and denote the\n",
    "proposed value of the parameter as $\\theta_{pro} = \\theta_{cur}+\\Delta\\theta$.  \n",
    "**2.** Compute the probability of moving to the proposed value as Equation 7.1, specifically expressed here as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_04_05.png\"  width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If the proposed value $\\theta_{pro}$ happens to land outside the permissible bounds of $\\theta$, the\n",
    "prior and/or likelihood is set to zero, hence $p_{move}$ is zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "**3.** Accept the proposed parameter value if a random value sampled froma [0, 1] uniform\n",
    "distribution is less than $p_{move}$, otherwise reject the proposed parameter value and tally\n",
    "the current value again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Repeat the above steps until it is judged that a sufficiently representative sample has been generated.   \n",
    "The judgment of “sufficiently representative” is not trivial and is an issue that will be discussed later in this chapter.   \n",
    "For now, the goal is to understand the mechanics of the Metropolis algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 7.4 shows specific examples of the Metropolis algorithm applied to a case with a $beta(\\theta|1, 1)$ prior, N =20, and z=14.   \n",
    "There are three columns in Figure 7.4, for three different runs of the Metropolis algorithm using three different values for $\\sigma$ in\n",
    "the proposal distribution.   \n",
    "In all three cases, $\\theta$ was arbitrarily started at 0.01, merely for\n",
    "purposes of illustration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_05.png\"  width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The middle column of Figure 7.4 uses a moderately sized SD for the proposal\n",
    "distribution, namely $\\sigma =0.2$, as indicated in the title of the upper-middle panel where\n",
    "it says “Prpsl.SD=0.2.”   \n",
    "This means that at any step in the chain, for whatever value $\\theta$\n",
    "happens to be, the proposed jump is within ±0.2 of $\\theta$ about 68% of the time (because a\n",
    "normal distribution has about 68% of its mass between −1 and +1 SDs).   \n",
    "In this case, the proposed jumps are accepted roughly half the time, as indicated in the center panel by the\n",
    "annotation $N_{acc}/N_{pro} =0.495$, which is the number of accepted proposals divided by the total number of proposals in the chain. This setting of the proposal distribution allows\n",
    "the chain to move around parameter space fairly efficiently. In particular, you can see\n",
    "in the lower-middle panel that the chain moves away quickly from the unrepresentative\n",
    "starting position of $\\theta = 0.01$.   \n",
    "And, the chain visits a variety of representative values in\n",
    "relatively few steps. That is, the chain is not very clumpy.   \n",
    "The upper-middle panel shows\n",
    "a histogram of the chain positions after 50,000 steps.   \n",
    "The histogram looks smooth and is\n",
    "an accurate approximation of the true underlying posterior distribution, which we know in this case is a $beta(\\theta|15, 7)$ distribution.   \n",
    "Although the chain is not very clumpy and yields a smooth-looking histogram, it does have some clumpiness because each step is linked to the location of the previous step, and about half the steps don’t change at all.   \n",
    "Thus, there are not 50,000 independent representative values of the\n",
    "posterior distribution in the chain.   \n",
    "If we take into account the clumpiness, then the so-called “effective size” of the chain is less, as indicated in the title of upper-middle\n",
    "panel where it says “Eff.Sz. = 11723.9.”   \n",
    "This is the equivalent number of values if they\n",
    "were sampled independently of each other.   \n",
    "The technical meaning of effective size will be discussed later in the chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The left column of Figure 7.4 uses a relatively small proposal SD, namely $\\theta = 0.02$, as indicated in the title of the upper-left panel where it says “Prpsl.SD = 0.02.”   \n",
    "You can see that successive steps in the chain make small moves because the proposed jumps are small.   \n",
    "In particular, in the lower-left panel you can see that it takes many steps for the chain to move away from the unrepresentative starting position of $\\theta = 0.01$.   \n",
    "The chain explores values only very gradually, producing a snake-like chain that lingers around particular values, thereby producing a form of clumpiness.  \n",
    "In the long run, the chain will explore the posterior distribution thoroughly and produce a good representation, but it will require a very long chain. The title of the upper-left panel indicates that the effective size of this 50,000 step chain is only 468.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The right column of Figure 7.4 uses a relatively large proposal SD, namely $\\theta = 2$, as indicated in the title of the upper-left panel where it says “Prpsl.SD = 2.”   \n",
    "The proposed jumps are often far away from the bulk of the posterior distribution, and therefore, the proposals are often rejected and the chain stays at one value for many steps.   \n",
    "The process accepts new values only occasionally, producing a very clumpy\n",
    "chain.   \n",
    "In the long run, the chain will explore the posterior distribution thoroughly\n",
    "and produce a good representation, but it will require a very long chain.   \n",
    "The title of the upper-right panel indicates that the effective size of this 50,000 step chain is  only 2113.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Regardless of the which proposal distribution in Figure 7.4 is used, the Metropolis algorithm will eventually produce an accurate representation of the posterior distribution, as is suggested by the histograms in the upper row of Figure 7.4.   \n",
    "What differs is the efficiency of achieving a good approximation.   \n",
    "The moderate proposal distribution will achieve a good approximation in fewer steps than either of the extreme proposal distributions.  \n",
    "Later in the chapter, we will discuss criteria for deciding that a chain has\n",
    "produced a sufficiently good approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## 7.4. TOWARD GIBBS SAMPLING: ESTIMATING TWO COIN BIASES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Metropolis method is very useful, but it can be inefficient.   \n",
    "Other methods can be more efficient in some situations.   \n",
    "In particular, another type of sampling method that can be very efficient is Gibbs sampling.   \n",
    "Gibbs sampling typically applies to models with multiple parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### 7.4.4. Gibbs sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The Metropolis algorithm is very general and broadly applicable.  \n",
    "One problem with it, however, is that the proposal distribution must be properly tuned to the posterior distribution if the algorithm is to work well.   \n",
    "If the proposal distribution is too narrow or too broad, a large proportion of proposed jumps will be rejected and the trajectory will\n",
    "get bogged down in a localized region of the parameter space.   \n",
    "Even at its most efficient, the effective size of the chain is far less than the number of proposed jumps.   \n",
    "It would be nice, therefore, if we had another method of sample generation that was more efficient.  \n",
    "Gibbs sampling can be especially useful for hierarchical models, which we explore in Chapter 9.   \n",
    "It turns out that Gibbs sampling is a special case of the Metropolis-Hastings algorithm, which is a generalized form of the Metropolis algorithm.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The procedure for Gibbs sampling is a type of random walk through parameter space, like the Metropolis algorithm.   \n",
    "The walk starts at some arbitrary point, and at each point\n",
    "in the walk, the next step depends only on the current position, and on no previous positions.  \n",
    "What is different about Gibbs sampling, relative to the Metropolis algorithm, is how each step is taken.   \n",
    "At each point in the walk, one of the component parameters\n",
    "is selected.   \n",
    "The component parameter could be selected at random, but typically the\n",
    "parameters are cycled through, in order: $\\theta_1, \\theta_2, \\theta_3, ... , \\theta_1, \\theta_2, \\theta_3, ...$.   \n",
    "The reason that parameters are cycled rather than selected randomly is that for complex models with many dozens or hundreds of parameters, it would take too many steps to visit every parameter by random chance alone, even though they would be visited about equally often in the long run.   \n",
    "Suppose that parameter $\\theta_i$ has been selected.  \n",
    "Gibbs sampling then chooses a new value for that parameter by generating a random value directly from\n",
    "the conditional probability distribution $p(\\theta_i|\\{\\theta_{j\\neq i}\\}, D)$.   \n",
    "The new value for $\\theta_i$, combined with the unchanged values of $\\theta_{j\\neq i}$, constitutes the new position in the random walk.  \n",
    "The process then repeats: Select a component parameter and select a new value for that parameter from its conditional posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img style=\"float: left;\" src=\"pic3/07_08.png\"  width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Figure 7.7 illustrates this process for a two-parameter example.    \n",
    "In the first step, we want to select a new value for $\\theta_1$.   \n",
    "We conditionalize on the values of all the other\n",
    "parameters, $\\theta_{j\\neq 1}$, from the previous step in the chain. In this example, there is only one other parameter, namely $\\theta_2$.   \n",
    "The upper panel of Figure 7.7 shows a slice through\n",
    "the joint distribution at the current value of $\\theta_2$.  \n",
    "The heavy curve is the posterior distribution conditional on this value of $\\theta_2$, denoted $p(\\theta_1|\\{\\theta_{j\\neq 1}\\}, D)$, which is $p(\\theta_i|\\theta_2, D)$\n",
    "in this case because there is only one other parameter.   \n",
    "If the mathematical form of the conditional distribution is appropriate, a computer can directly generate a random\n",
    "value of $\\theta_1$.   \n",
    "Having thereby generated a new value for $\\theta_1$, we then conditionalize on it and determine the conditional distribution of the next parameter, $\\theta_2$, as shown in the lower panel of Figure 7.7.   \n",
    "The conditional distribution is denoted formally as\n",
    "$p(\\theta_2|\\{\\theta_{j\\neq 2}\\}, D)$, which equals $p(\\theta_2|\\theta_1, D)$ in this case of only two parameters.  \n",
    "If the mathematical form is convenient, our computer can directly generate a random value of $\\theta_2$ from the conditional distribution.   \n",
    "We then conditionalize on the new value $\\theta_2$, and the cycle repeats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gibbs sampling can be thought of as just a special case of the Metropolis algorithm, in which the proposal distribution depends on the location in parameter space and the component parameter selected.  \n",
    "At any point, a component parameter is selected, and\n",
    "then the proposal distribution for that parameter’s next value is the conditional posterior\n",
    "probability of that parameter.  \n",
    "Because the proposal distribution exactly mirrors the posterior\n",
    "probability for that parameter, the proposed move is always accepted.   \n",
    "A rigorous proof of this idea requires development of a generalized form of the Metropolis algorithm, called the\n",
    "Metropolis-Hastings algorithm (Hastings, 1970). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Gibbs sampling is especially useful when the complete joint posterior, $p(\\{\\theta_i\\}|D)$, cannot be analytically determined and cannot be directly sampled, but all the conditional\n",
    "distributions, $p(\\theta_i|\\{\\theta_{j\\neq i}\\}, D)$, can be determined and directly sampled.  \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
