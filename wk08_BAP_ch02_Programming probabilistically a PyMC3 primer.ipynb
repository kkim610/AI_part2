{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Chapter 2: Progeamming Probabilistically - PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "conda install -c conda-forge theano "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pip install arviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "conda install -c conda-forge pymc3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "from arviz import plot_trace\n",
    "import arviz as az\n",
    "import seaborn as sns\n",
    "palette = 'muted'\n",
    "sns.set_palette(palette); sns.set_color_codes(palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Probabilistic programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Bayesian statistics is conceptually very simple:   \n",
    "we have some data that is fixed, in the sense that we cannot change what we have measured,   \n",
    "and we have parameters whose values are of interest to us and hence we explore their plausible values.  \n",
    "\n",
    "All the uncertainties we have are modeled using probabilities.   \n",
    "In other statistical paradigms, there are different types of unknown quantities;   \n",
    "in the Bayesian framework everything that is unknown is treated the same.   \n",
    "If we do not know a quantity we assign a probability distribution to it. \n",
    "\n",
    "Then, Bayes' theorem is used   \n",
    "to transform the prior probability distribution $p(θ)$  (what we know about a given problem before observing the data),   \n",
    "into a posterior distribution $p(θ|D)$ (what we know after observing data). \n",
    "\n",
    "In other words, Bayesian statistics is a form of learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The possibility of automating the inference part has led to the development of\n",
    "**probabilistic programming languages (PPL) that allow for a clear separation\n",
    "between model creation and inference.**  \n",
    "\n",
    "In the PPL framework, users specify a\n",
    "full probabilistic model by writing a few lines of code and then inference follows\n",
    "automatically.  \n",
    "It is expected that probabilistic programming will have a major impact\n",
    "on data science and other disciplines by enabling practitioners to build complex\n",
    "probabilistic models in a less time-consuming and less error-prone way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Probabilistic programming hides from the user details on how probabilities are manipulated and\n",
    "how the inference is performed, allowing users to focus on model specification and\n",
    "analysis of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Inference engines\n",
    "\n",
    "There are several methods to compute the posterior even when it is not possible to\n",
    "solve it analytically. Some of the methods are:  \n",
    "* **Non-Markovian methods:**\n",
    "    * Grid computing\n",
    "    * Quadratic approximation\n",
    "    * Variational methods\n",
    "\n",
    "* **Markovian methods:**\n",
    "    * Metropolis-Hastings\n",
    "    * Hamiltonian Monte Carlo/No U-Turn Sampler\n",
    "\n",
    "Nowadays, Bayesian analysis is performed mainly by using Markov Chain Monte Carlo (MCMC) methods, with variational methods gaining momentum for bigger datasets.   \n",
    "We do not need to really understand these methods to perform\n",
    "Bayesian analysis, that's the whole point of probabilistic programming languages,\n",
    "but knowing at least how they work at a conceptual level is often very useful, for\n",
    "example for debugging our models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Grid computing\n",
    "\n",
    "Grid computing is a brute-force approach.   \n",
    "Even if you are not able to compute the\n",
    "whole posterior, you may be able to compute the prior and the likelihood for a\n",
    "given number of points.   \n",
    "Let's assume we want to compute the posterior for a single\n",
    "parameter model.   \n",
    "The grid approximation is as follows:\n",
    "1. Define a reasonable interval for the parameter (the prior should give\n",
    "you a hint).\n",
    "2. Place a grid of points (generally equidistant) on that interval.\n",
    "3. For each point in the grid we multiply the likelihood and the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def posterior_grid(grid_points=100, heads=6, tosses=9):\n",
    "    \"\"\"\n",
    "    A grid implementation for the coin-flip problem\n",
    "    \"\"\"\n",
    "    # define a grid\n",
    "    grid = np.linspace(0, 1, grid_points)\n",
    "\n",
    "    # define prior\n",
    "    prior = np.repeat(5, grid_points)  # uniform\n",
    "    #prior = (grid  <= 0.4).astype(int)  # truncated\n",
    "    #prior = abs(grid - 0.5)  # \"M\" prior\n",
    "\n",
    "    # compute likelihood at each point in the grid\n",
    "    likelihood = stats.binom.pmf(heads, tosses, grid)\n",
    "\n",
    "    # compute product of likelihood and prior\n",
    "    unstd_posterior = likelihood * prior\n",
    "\n",
    "    # standardize the posterior, so it sums to 1\n",
    "    posterior = unstd_posterior / unstd_posterior.sum()\n",
    "    return grid, posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "points = 15\n",
    "h, n = 1, 4\n",
    "grid, posterior = posterior_grid(points, h, n)\n",
    "plt.plot(grid, posterior, 'o-')\n",
    "plt.plot(0, 0, label='heads = {}\\ntosses = {}'.format(h, n), alpha=0)\n",
    "plt.xlabel(r'$\\theta$', fontsize=14)\n",
    "plt.legend(loc=0, fontsize=14)\n",
    "plt.savefig('B04958_02_01.png', dpi=300, figsize=(5.5, 5.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Quadratic method\n",
    "\n",
    "The quadratic approximation, also known as the Laplace method or the normal\n",
    "approximation, consists of approximating the posterior with a Gaussian distribution.  \n",
    "This method often works because in general the region close to the mode of the\n",
    "posterior distribution is more or less normal, and in fact in many cases is actually a\n",
    "Gaussian distribution.   \n",
    "This method consists of two steps.   \n",
    "First, find the mode of the posterior distribution.   \n",
    "This can be done using optimization methods; that is, methods\n",
    "to find the maximum or minimum of a function, and there are many off-the-shelf\n",
    "methods for this purpose.  \n",
    "This will be the mean of the approximating Gaussian.\n",
    "Then we can estimate the curvature of the function near the mode.  \n",
    "Based on this curvature, the standard deviation of the approximating Gaussian can be computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Variational methods\n",
    "\n",
    "Most of modern Bayesian statistics is done using Markovian methods (see the next\n",
    "section), but for some problems those methods can be too slow and they do not\n",
    "necessarily parallelize well.  \n",
    "The naive approach is to simply run n chains in parallel\n",
    "and then combine the results, but for many problems this is not a really good\n",
    "solution.   \n",
    "Finding effective ways of parallelizing them is an active research area.  \n",
    "Variational methods could be a better choice for large datasets (think big data)\n",
    "and/or for likelihoods that are too expensive to compute.   \n",
    "In addition, these\n",
    "methods are useful for quick approximations to the posterior and as starting points\n",
    "for MCMC methods.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The general idea of variational methods is to approximate the posterior with a\n",
    "simpler distribution; this may sound similar to the Laplace approximation, but the\n",
    "similarities vanish when we check the details of the method.   \n",
    "The main drawback\n",
    "of variational methods is that we must come up with a specific algorithm for each\n",
    "model, so it is not really a universal inference engine, but a model-specific one.  \n",
    "Of course, lots of people have tried to automatize variational methods. A recently\n",
    "proposed method is the automatic differentiation variational inference (ADVI)\n",
    "(see http://arxiv.org/abs/1603.00788).   \n",
    "ADVI is already implemented on PyMC3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## MCMC\n",
    "To understand what\n",
    "MCMC methods are we are going to split the method into the two MC parts, the\n",
    "Monte Carlo part and the Markov Chain part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Monte Carlo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Monte Carlo methods are a very broad family of algorithms that use random sampling to compute or simulate a given process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Monte Carlo is a very famous casino located in the Principality of Monaco.\n",
    "One of the developers of the Monte Carlo method, Stanislaw Ulam, had an uncle who used to gamble there.\n",
    "The key idea Stan had was that while many problems are difficult to solve or even formulate in an exact way, they can be effectively studied by taking samples from them, or by simulating them.\n",
    "In fact, the motivation was to answer questions about the probability of getting a particular hand in a solitary game.\n",
    "One way to solve this problem was to follow the analytical combinatorial problem.\n",
    "Another way, Stan argued, was to play several games of solitaire and just count how many of the hands we play match the particular hand we are interested in! Maybe this sounds obvious, or at least pretty reasonable; for example, you may have used re-sampling methods to solve your statistical problems.\n",
    "But remember this mental experiment was performed about 70 years ago, a time when the first practical computers began to be developed.\n",
    "The first application of the method was to solve a problem of nuclear physics, a problem really hard to tackle using the conventional tools at that time.\n",
    "Nowadays, even personal computers are powerful enough to solve many interesting problems using the Monte Carlo approach and hence these methods are applied to a wide variety of problems in science, engineering, industry, arts, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A classic  example of using a Monte Carlo method to compute a quantity\n",
    "of interest is the numerical estimation of $\\pi$.   \n",
    "In practice there are better methods for\n",
    "this particular computation, but its pedagocial value still remains.   \n",
    "We can estimate\n",
    "the value of $\\pi$ with the following procedure:  \n",
    "1. Throw $N$ points at random into a square of side $2R$.\n",
    "2. Draw a circle of radius $R$ inscribed in the square and count the number of\n",
    "points that are **inside** that circle.\n",
    "3. Estimate $\\hat{\\pi}$ as the ratio  $\\frac{N×inside}{4}$  \n",
    "A couple of notes: We know a point is inside a circle if the following relation is true:\n",
    "$$\\sqrt{x^2 + y^2} ≤ R$$\n",
    "\n",
    "The area of the square is $(2R)^2$ and the area of the circle is ${\\pi}R^2$.   \n",
    "Thus we\n",
    "know that the ratio of the area of the square to the area of the circle is $\\frac{4}{\\pi}$, and the area\n",
    "of the circle and squares are proportional to the number of points inside the circle\n",
    "and the total $N$ points, respectively.\n",
    "\n",
    "Using a few lines of Python we can run this simple Monte Carlo simulation and\n",
    "compute $\\pi$ and also the relative error of our estimate compared to the true\n",
    "value of $\\pi$ :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "N = 10000\n",
    "\n",
    "x, y = np.random.uniform(-1, 1, size=(2, N))\n",
    "inside = (x**2 + y**2)  <= 1\n",
    "pi = inside.sum()*4/N\n",
    "error = abs((pi - np.pi)/pi)* 100\n",
    "\n",
    "outside = np.invert(inside)\n",
    "\n",
    "plt.plot(x[inside], y[inside], 'b.')\n",
    "plt.plot(x[outside], y[outside], 'r.')\n",
    "plt.plot(0, 0, label='$\\hat \\pi$ = {:4.3f}\\nerror = {:4.3f}%'.format(pi, error), alpha=0)\n",
    "plt.axis('square')\n",
    "plt.legend(frameon=True, framealpha=0.9, fontsize=16);\n",
    "plt.savefig('B04958_02_02.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Markov chain\n",
    "A Markov chain is a mathematical object consisting of a sequence of states and a set\n",
    "of probabilities describing the transitions among those states.   \n",
    "A chain is Markovian\n",
    "if the probability of moving to other states depends only on the current state.  \n",
    "Given such a chain, we can perform a random walk by choosing a starting point\n",
    "and moving to other states following the transition probabilities.   \n",
    "If we somehow\n",
    "find a Markov chain with transitions proportional to the distribution we want to\n",
    "sample from (the posterior distribution in Bayesian analysis), sampling becomes\n",
    "just a matter of moving between states in this chain.   \n",
    "So, how do we find this chain if we do not know the posterior in the first place?   \n",
    "Well, there is something known as **detailed balance condition**.  \n",
    "Intuitively, this condition says that we should move in\n",
    "a reversible way (a reversible process is a common approximation in physics). That\n",
    "is, the probability of being in state i and moving to state j should be the same as the\n",
    "probability of being in state j and moving towards state i.\n",
    "\n",
    "In summary, all this means that if we manage to create a Markov Chain satisfying\n",
    "detailed balance we can sample from that chain with the guarantee that we will get\n",
    "samples from the correct distribution.   \n",
    "This is a truly remarkable result!   \n",
    "The most\n",
    "popular method that guarantees detailed balance is the Metropolis-Hasting algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Metropolis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def metropolis(func, steps=10000):\n",
    "    \"\"\"A very simple Metropolis implementation\"\"\"\n",
    "    samples = np.zeros(steps)\n",
    "    old_x = func.mean()\n",
    "    old_prob = func.pdf(old_x)\n",
    "\n",
    "    for i in range(steps):\n",
    "        new_x = old_x + np.random.normal(0, 1)\n",
    "        new_prob = func.pdf(new_x)\n",
    "        acceptance = new_prob/old_prob\n",
    "        if acceptance >= np.random.random():\n",
    "            samples[i] = new_x\n",
    "            old_x = new_x\n",
    "            old_prob = new_prob\n",
    "        else:\n",
    "            samples[i] = old_x\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.random.seed(345)\n",
    "func = stats.beta(0.4, 2)\n",
    "samples = metropolis(func=func)\n",
    "x = np.linspace(0.01, .99, 100)\n",
    "y = func.pdf(x)\n",
    "plt.xlim(0, 1)\n",
    "plt.plot(x, y, 'r-', lw=3, label='True distribution')\n",
    "plt.hist(samples, bins=30, density=True, stacked=True, label='Estimated distribution')\n",
    "plt.xlabel('$x$', fontsize=14)\n",
    "plt.ylabel('$pdf(x)$', fontsize=14)\n",
    "plt.legend(fontsize=14)\n",
    "plt.savefig('B04958_02_03.png', dpi=300, figsize=(5.5, 5.5));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# PyMC3 introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "PyMC3 is a Python library for probabilistic programming. PyMC3 provides a very simple and intuitive syntax that is easy to read and that is close to the syntax used in the statistical literature to describe probabilistic models. PyMC3 is written using Python, where the computationally demanding parts are written using NumPy and Theano. Theano is a Python library originally developed for deep learning that allows us to define, optimize, and evaluate mathematical expressions involving multidimensional arrays efficiently. The main reason PyMC3 uses Theano is because some of the sampling methods, like NUTS, need gradients to be computed and Theano knows how to do automatic differentiation. Also, Theano compiles Python code to C code, and hence PyMC3 is really fast. This is all the information about Theano we need to have to use PyMC3. If you still want to learn more about it start reading the official Theano tutorial at http://deeplearning.net/software/theano/tutorial/index.html#tutorial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Coin-flipping, the computational approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Let's revisit the coin-flipping problem, but this time using PyMC3. The first requirement is to get our data. We will use the same synthetic data as before. Since we are generating the data we know the value of $\\theta$, <span style=\"font-family: Courier New; font-size: 1.15em;\">theta_real</span> in the following code. Of course, for a real dataset we would not have this knowledge and in fact this is what we want to estimate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n_experiments = 4\n",
    "theta_real = 0.35  # unkwon value in a real experiment\n",
    "data = stats.bernoulli.rvs(p=theta_real, size=n_experiments)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Model specification "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Now that we have the data, we need to specify the model. Remember this is done by specifying the likelihood and the prior using probability distributions. As the likelihood, we will use the binomial distribution with parameter $n=1$ and $p=\\theta$, and for the prior a beta with $\\alpha = \\beta =1$. This beta distribution is equivalent to a uniform distribution in the interval [0,1]. We can write the model using mathematical notation as follows: \n",
    "\n",
    "$\\theta \\sim Beta(\\alpha=1, \\beta=1)$  \n",
    "$y \\sim Bin(n=1, p=\\theta)$  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "This statistical model has an almost one-to-one translation to the PyMC3 syntax.   \n",
    "* The first line of the code creates a container for our first model, PyMC3 uses the <span style=\"font-family: Courier New; font-size: 1.15em;\">with</span> statement to indicate that everything inside the with block points to the same model.   \n",
    "You can think of this as syntactic sugar to ease model specification. Imaginatively, our model is called <span style=\"font-family: Courier New; font-size: 1.15em;\">our_first_model</span>. \n",
    "* The second line specifies the prior, and, as you can see, the syntax follows the mathematical notation closely.   \n",
    "We call the random variable <span style=\"font-family: Courier New; font-size: 1.15em;\">theta</span>.   \n",
    "Please note that this name matches the first argument of the PyMC3 <span style=\"font-family: Courier New; font-size: 1.15em;\">Beta</span> function; having both names the same is a good practice to avoid confusion.   \n",
    "Then we will use the name of the variable to extract information from the sampled posterior.   \n",
    "The variable <span style=\"font-family: Courier New; font-size: 1.15em;\">theta</span> is a stochastic variable; we can think of this variable as the rule to generate numbers from a given distribution (a beta distribution in this case) and not actual numbers.   \n",
    "* The third line specifies the likelihood following the same syntax as for the prior except that we pass the data using the <span style=\"font-family: Courier New; font-size: 1.15em;\">observed</span> argument.   \n",
    "This is the way in which we tell PyMC3 that this is the likelihood.   \n",
    "The data can be a Python list, a NumPy array or a Pandas DataFrame.   \n",
    "That's all it takes to specify our model!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as our_first_model:\n",
    "    # a priori\n",
    "    theta = pm.Beta('theta', alpha=1, beta=1)\n",
    "    # likelihood\n",
    "    y = pm.Bernoulli('y', p=theta, observed=data)\n",
    "    \n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(1000, step=step)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Pushing the inference button "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "For this problem the posterior can be computed analytically and we can also take samples form the posterior using PyMC3 with just a few lines.   \n",
    " \n",
    "* The fourth line is used to define the sampling method.   \n",
    "Here we are using Metropolis-Hastings, simply called Metropolis.  \n",
    "PyMC3 allows us to assign different samplers to different random variables; for now we have a model with only one parameter, but later we will have more variables.   \n",
    "Alternatively, we can even omit this line and PyMC3 will assign samplers automatically to each variable based on properties of those variables.   \n",
    "For example, NUTS works only for continuous variables and hence cannot be used with a discrete one, Metropolis can deal with discrete variables, and other types of variables have specially dedicated samplers.   \n",
    "In general we should let PyMC3 choose the sampler for us.   \n",
    "* The last line performs the inference.   \n",
    "The first argument is the number of samples we want, and the second and third arguments are the sampling method and the starting point. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Diagnosing the sampling process "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Since we are approximating the posterior with a finite number of samples, the first thing we need to do is to check if we have a reasonable approximation. There are several tests we can run, some are visual and some quantitative. These tests try to find problems with our samples but they cannot prove we have the correct distribution; they can only provide evidence that the sample seems reasonable. If we find problems with the sample, the solutions are: \n",
    "* Increase the number of samples.\n",
    "* Remove a number of samples from the beginning of the trace. This is know as burn-in. MCMC methods often take some time until we start getting samples from the target distribution. The burn-in will not be necessary for an infinite sample, as it is not part of the Markovian theory. Instead, removing the first samples is an ad hoc trick to get better results given that we are getting a finite sample. Remember we should not get confused by mixing mathematical objects with the approximation of those objects. Spheres, Gaussian, Markov chains, and all the mathematical objects live only in the platonic world of the ideas, not in our imperfect but real world. \n",
    "* Re-parametrize the model, that is express the model in a different but equivalent way. \n",
    "* Transform the data. This can help a lot to get a much more efficient sampling. When transforming the data we should take care to interpret the result in the transformed space or, alternatively, revert the transformation before interpreting the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Convergence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Generally, the first task we will perform is to check what the results look like.  The traceplot function is ideally suited to this task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "burnin = 100  \n",
    "chain = trace[burnin:]\n",
    "az.plot_trace(multi_chain, compact=False, var_names=('theta'))\n",
    "plt.savefig('B04958_02_04.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We get two plots for each unobserved variable. On the left, we get a kernel density estimation (KDE) plot; this is like the smoothed version of a histogram. On the right, we get the individual sampled values at each step during the sampling. Notice that the read line is indicating the value of the variable <span style=\"font-family: Courier New; font-size: 1.15em;\">theta_real</span>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "What do we need to look at when we see these plots?  \n",
    "Well, KDE plots should look like smooth curves.   \n",
    "Often, as the number of data increases, the distribution of each parameter will tend to become Gaussian-like; this is due to the law of the large numbers.   \n",
    "Of course, this is not always true.   \n",
    "The plot on the right should look like white noise; we are looking for good mixing.   \n",
    "We should not see any recognizable pattern, we should not see a curve going up or down, instead we want a curve meandering around a single value.   \n",
    "For multimodal distributions or discrete distributions we expect the curve to not spend too much time in a value or region before moving to other regions, we want to see sampled values moving freely among these regions.   \n",
    "We should also expect to see a stable auto-similar trace, that is, a trace that at different points looks more or less the same; for example, the first 10% (or so) should look similar to other portions in the trace like the last 50% or 10%.   \n",
    "Once again, we do not want patterns; instead we expect something noisy.   \n",
    "See the following figure for some examples of traces with good mixing (on the right) and bad mixing (on the left):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "<img src=\"pic2\\fig_2_1.png\" width=\"700\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "If the first part of the trace looks different than the others this is an indication of the need for burnin. If we see a lack of auto-similarity in other parts or we see some pattern this is an indication for more steps, or the need to use a different sampler or a different parametrization. For difficult models, we may apply a combination of all these strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "PyMC3 allows us to run a model several times in parallel and thus get a parallel chain for the same parameter. This is specified with the argument njobs in the sample function. Using traceplot, we plot all the chains for the same parameter in the same plot. Since each chain is independent of the others and each chain should be a good sample, all the chain should look similar to each other. Besides checking for convergence, these parallel chains can be used also for inference; instead of discarding the extra chains, we can combine them to increase the sample size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with our_first_model:\n",
    "    step = pm.Metropolis()\n",
    "    multi_trace = pm.sample(1000, step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "burnin = 0  # no burnin\n",
    "multi_chain = multi_trace[burnin:]\n",
    "#pm.traceplot(multi_chain, lines={'theta':theta_real});\n",
    "az.plot_trace(multi_chain, compact=False, var_names=('theta'))\n",
    "plt.savefig('B04958_02_06.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A quantitative way to check for convergence is by using the Gelman-Rubin test. The idea of this test is to compare the variance between chains with the variance within chains, so of course we need more than one chain for this test to work. Ideally, we should expect a value of $\\hat{R} = 1$. As an empirical rule, we will be ok with a value below 1.1; higher values are signaling a lack of convergence:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_posterior(multi_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(multi_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(multi_chain, hdi_prob=0.95)\n",
    "plt.savefig('az20211207.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The function <span style=\"font-family: Courier New; font-size: 1.15em;\">summary</span> provides a text-format summary of the posterior. We get the mean, standard deviation, and the HPD intervals:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pm.summary(multi_chain) # hpd_2.5 부터  hpd_97.5  --> 95% Credible Interval "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "One of the quantities returned is the <span style=\"font-family: Courier New; font-size: 1.15em;\">mc_error</span>. This is an estimation of the error introduced by the sampling method. The estimation takes into account that the samples are not truly independent of each other. The <span style=\"font-family: Courier New; font-size: 1.15em;\">mc_error</span> is the standard error of the means $x$ of $n$ blocks, each block is just a portion of the trace: \n",
    "$$MC_{error} = \\frac {σ(n)}{\\sqrt x} $$\n",
    "\n",
    "This error should be of course below the precision we want in our results. Since the sampling methods are stochastic, every time we re-run our models the values returned by <span style=\"font-family: Courier New; font-size: 1.15em;\">summary</span>  will be different; nevertheless, they should be similar for different runs. If they are not as similar as we want we may need more samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Autocorrelation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "An ideal sample will lack autocorrelation, that is, a value at one point should be independent of the values at other points. In practice, samples generated from MCMC methods, especially Metropolis-Hastings, can be autocorrelated. Some models will also lead to more autocorrelated samples due to correlations in the way one parameter depends on the others. PyMC3 comes with a convenient function to plot the autocorrelation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pm.autocorrplot(chain)\n",
    "plt.savefig('B04958_02_08.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The plot shows the average correlation of sample values compared to successive points (up to 100 points). Ideally we should see no autocorrelation, in practice; we seek samples that quickly drop to low values of autocorrelation. The more autocorrelated a parameter is, the larger the number of samples we will need to obtain a given precision; that is, autocorrelation has the detrimental effect of lowering the effective number of samples. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Effective size "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "A sample with autocorrelation has less information than a sample of the same size without autocorrelation. Hence, given a sample of a certain size with a certain degree of autocorrelation we could try to estimate what will be the size of the sample with the same information without autocorrelation. That number will be the effective size of the sample. Ideally both quantities should be the same; the closer the two numbers the more efficient our sampling. The effective size of sample could serve us as a guide. If we want to estimate mean values of a distribution we will need an effective sample of at least 100 samples; if we want to estimate quantities that depend on the tails of distribution, such as the limits of credible intervals, we will need an effective size of 1000 to 10000 samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pm.effective_n(multi_chain)['theta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "One way to have more efficient sampling is of course to use a better sampling method. An alternative is to transform the data or re-parametrize the model. Another commonly used option in the literature is to thin a chain. Thinning is just taking every k-esim observation. In Python, we would say taking slices of a chain. Thinning will indeed reduce the autocorrelation, but at the expense of reducing the number of samples. So in practice, it is generally a better idea to just increase the number of samples instead of doing thinning. Nonetheless, thinning can be useful, for example, to reduce storage requirements. When high autocorrelation cannot be avoided, we are obligated to compute long chains, and if our models contain many parameters storage can become problematic. Also we may need to do some post-processing of the posterior such as performing some expensive computation. In such cases having a smaller sample of minimally autocorrelated values could be important. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "All the diagnostic tests we have seen have an empirical component and none of them is definitive. In practice, we run several tests and, if all of them look OK, then we proceed to further analyses. If we detect problems we have to go back and fix them; this is just part of the iterative process of modeling. It is also important to notice that having to run convergence tests is not really part of the Bayesian theory but is about the Bayesian practice, given that we are computing the posterior using numerical methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Summarizing the posterior "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "As we have already seen, the result of a Bayesian analysis is a posterior distribution. This contains all the information about our parameters, according to the data and the model. One way to visually summarize the posterior is to use the <span style=\"font-family: Courier New; font-size: 1.15em;\">plot_posterior</span> function that comes with PyMC3. This function accepts a PyMC3 trace or a NumPy array as a main argument. By default, <span style=\"font-family: Courier New; font-size: 1.15em;\">plot_posterior</span> shows a histogram for the credible parameters together with the mean of the distribution and the 94% HPD as a thick black line at the bottom of the plot. Different interval values can be set for the HPD with the argument <span style=\"font-family: Courier New; font-size: 1.15em;\">credible_interval</span>. We are going to refer to this type of plot as Kruschke's plot, since John K. Kruschke introduced this type of plot in his great book Doing Bayesian Data Analysis:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pm.plot_posterior(chain, kde_plot=True)   # 기본 값 credible_interval=0.94 \n",
    "plt.savefig('B04958_02_09.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pm.plot_posterior(chain, kde_plot=True, credible_interval=0.5 )   \n",
    "plt.savefig('B04958_02_09.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Posterior-based decisions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "Sometimes describing the posterior is not enough. Sometimes we need to make decisions based on our inferences. We have to reduce a continuous estimation to a dichotomous one: yes or no, contaminated or safe, and so on. Back to our problem, we may need to decide if the coin is fair or not fair. A fair coin is one with a $θ$ value of exactly 0.5. Strictly speaking, the chance of such a result is zero (think of an infinite number of trailing zeros), hence in practice we relax our definition of fairness and we will say a fair coin is one with a value of $θ$ around 0.5. What around exactly means is context-dependent; there is no auto-magic rule that will fit everyone's intentions. Decisions are inherently subjective and our mission is to take the most informed possible decisions according to our goals. Intuitively, one way to take such an informed decision is to compare the HPD to the value of interest, 0.5 in our case. In the preceding figure, we can see that the HPD goes from ~ 0.06 to ~0.71 and hence 0.5 is included in the HPD. According to our posterior, the coin seems to be tail-biased, but we cannot completely rule out the possibility that the coin is fair; maybe if we want a sharper decision we will need to collect more data to reduce the spread of the posterior or maybe we missed some important information that we could use to define a more informative prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### ROPE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "One possible option to take a posterior-based decision is to define a Region Of Practical Equivalence (ROPE). This is just an interval around the value of interest; for example, we could say that any value in the interval [0.45, 0.55] will be, for our purposes, practically equivalent to 0.5. Once again the ROPE is context-dependent. So, now we are going to compare the ROPE to the HPD. \n",
    "\n",
    "We can define at least  three scenarios: \n",
    "* The ROPE does not overlap with the HPD, and hence we can say the coin  is not fair \n",
    "* The ROPE contains the entire HPD; we will say the coin is fair \n",
    "* The ROPE partially overlaps with HPD; we cannot say the coin is fair  or unfair \n",
    "\n",
    "Of course, if we choose a ROPE  to cover the entire interval [0, 1], we will always say we have a fair coin no matter what data we have but probably nobody is going to agree with our ROPE definition. \n",
    "\n",
    "The <span style=\"font-family: Courier New; font-size: 1.15em;\">plot_posterior</span> function can be used to plot a ROPE. The ROPE appears as a semi-transparent red and very thick line, together with the overlayed limits  of the ROPE:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pm.plot_posterior(chain, kde_plot=True, credible_interval=0.95, rope=[0.45, .55])\n",
    "plt.savefig('B04958_02_10.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "We can also pass to <span style=\"font-family: Courier New; font-size: 1.15em;\">plot_posterior</span> a reference value, for example 0.5, that we want to compare with the posterior. We will get a green vertical line and the proportion of the posterior above and below our reference value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "pm.plot_posterior(chain, kde_plot=True, credible_interval=0.95, ref_val=0.5)\n",
    "plt.savefig('B04958_02_11.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# likelihood:  Bin( ) vs. Bernoulli( )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "동전 던지기 문제에서 observed = data 의 data가 무엇이냐에 따라 pm.binomial( ) 과 pm.bernoulli( ) 중에서 선택한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "n_experiments = 4\n",
    "theta_real = 0.35  # unkwon value in a real experiment\n",
    "data2 = stats.bernoulli.rvs(p=theta_real, size=n_experiments)\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data2=data2.sum()\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as our_second_model:\n",
    "    # a priori\n",
    "    theta2 = pm.Beta('theta2', alpha=1, beta=1)\n",
    "    # likelihood\n",
    "    #y = pm.Bernoulli('y', p=theta, observed=data)\n",
    "    y2 = pm.Binomial('y2',n=n_experiments, p=theta2, observed=data2)\n",
    "    start2 = pm.find_MAP()\n",
    "    step2 = pm.Metropolis()\n",
    "    trace2 = pm.sample(1000, step=step2, start=start2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "burnin2 = 100  \n",
    "chain2 = trace2[burnin2:]\n",
    "#pm.traceplot(chain2, lines={'theta':theta_real});\n",
    "az.plot_trace(chain2, compact=False, var_names=('theta2'))\n",
    "plt.savefig('B04958_02_04.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pm.summary(chain2) # Binomial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "pm.summary(multi_chain) # Bernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "az.plot_posterior(chain2)\n",
    "plt.savefig('B04958_02_09.png', dpi=300, figsize=(5.5, 5.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import sys, IPython, scipy, matplotlib, platform\n",
    "print(\"This notebook was created on a %s computer running %s and using:\\nPython %s\\nIPython %s\\nPyMC3 %s\\nNumPy %s\\nSciPy %s\\nMatplotlib %s\\nSeaborn %s\" % (platform.machine(), ' '.join(platform.linux_distribution()[:2]), sys.version[:5], IPython.__version__, pm.__version__, np.__version__, scipy.__version__, matplotlib.__version__, sns.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# 여기까지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "with pm.Model() as our_first_model:\n",
    "    # a priori\n",
    "    theta = pm.Beta('theta', alpha=1, beta=1)\n",
    "    # likelihood\n",
    "    #y = pm.Bernoulli('y', p=theta, observed=data)\n",
    "    #y = pm.Binomial('y',n=n_experiments, p=theta, observed=sum(data))\n",
    "    y = pm.Binomial('y',n=1, p=theta, observed=data)\n",
    "    step = pm.Metropolis()\n",
    "    trace = pm.sample(10000, step=step)\n",
    "    #trace = pm.sample(1000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
